{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zPyAuRrJtDF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TNFnxe-OunN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmxQsYjtlZSd"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/Thesis/FSD50K /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv4X8PHN8VBr"
      },
      "outputs": [],
      "source": [
        "!rsync -av --progress /content/drive/MyDrive/Thesis/FSD50K /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ4T7WGQiGKE"
      },
      "outputs": [],
      "source": [
        "train_csv_path = '/content/FSD50K/FSD50K.metadata/collection/modified_collection_dev.csv'\n",
        "test_csv_path = '/content/FSD50K/FSD50K.metadata/collection/modified_collection_test.csv'\n",
        "train_audio_dir = '/content/FSD50K/FSD50K.dev_audio/'\n",
        "test_audio_dir = '/content/FSD50K/FSD50K.eval_audio/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEKl2LR2Y7mz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "3SApZzZHNWzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ioSFL9ZJoD"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "train_df_sampled = train_df.sample(n=20483, random_state=42)\n",
        "\n",
        "#Save the new CSV file with the reduced training dataset\n",
        "train_csv_sampled_path = '/content/FSD50K/FSD50K.metadata/collection/modified_collection_dev_20483_new.csv'\n",
        "train_df_sampled.to_csv(train_csv_sampled_path, index=False)\n",
        "\n",
        "\n",
        "print(f\"Reduced CSV file saved to: {train_csv_sampled_path}\")\n",
        "\n",
        "test_df_sampled = test_df.sample(n=5116, random_state=42)  # Adjust 'n' as needed\n",
        "\n",
        "#Save the new CSV file with the reduced test dataset\n",
        "test_csv_sampled_path = '/content/FSD50K/FSD50K.metadata/collection/modified_collection_test_5116_new.csv'\n",
        "test_df_sampled.to_csv(test_csv_sampled_path, index=False)\n",
        "\n",
        "print(f\"Reduced test CSV file saved to: {test_csv_sampled_path}\")\n",
        "\n",
        "#Update the DataFrame to be used in further processing\n",
        "train_df = train_df_sampled\n",
        "\n",
        "#Update the test DataFrame to be used in further processing\n",
        "test_df = test_df_sampled\n",
        "\n",
        "print(train_df['single_label'].dtype)\n",
        "print(train_df['single_label'].unique())\n",
        "\n",
        "train_df['single_label'] = train_df['single_label'].astype(str)\n",
        "test_df['single_label'] = test_df['single_label'].astype(str)\n",
        "\n",
        "\n",
        "unique_labels = sorted(train_df['single_label'].unique())\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "print(f\"New label mapping: {label_to_index}\")\n",
        "\n",
        "#Apply the mapping\n",
        "train_df['label_idx'] = train_df['single_label'].map(label_to_index)\n",
        "test_df['label_idx'] = test_df['single_label'].map(label_to_index)\n",
        "\n",
        "#Verify that the label indices are within the correct range\n",
        "print(train_df['label_idx'].value_counts())  # Check the distribution of label indices\n",
        "print(train_df['label_idx'].max())  # Ensure that the maximum index does not exceed num_classes - 1\n",
        "\n",
        "#Convert filenames to strings if necessary\n",
        "train_df['fname'] = train_df['fname'].astype(str)\n",
        "test_df['fname'] = test_df['fname'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gnwjFsBZQTP"
      },
      "outputs": [],
      "source": [
        "train_audio_dir = '/content/drive/MyDrive/Thesis/FSD50K/FSD50K.dev_audio/'\n",
        "test_audio_dir = '/content/drive/MyDrive/Thesis/FSD50K/FSD50K.eval_audio/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKPpi0mCZ3Gg"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "class FSD50KDataset(Dataset):\n",
        "    def __init__(self, csv_file, audio_dir, transform=None, max_len=1000):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "        self.max_len = max_len  #Maximum length to pad/truncate sequences\n",
        "        self.mel_spectrogram = transforms.MelSpectrogram(\n",
        "            sample_rate=16000,\n",
        "            n_mels=64,\n",
        "            n_fft=1024,\n",
        "            hop_length=512\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dataframe = self.dataframe.dropna(subset=['single_label'])\n",
        "\n",
        "\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(self.dataframe['single_label'].unique())}\n",
        "        self.dataframe['encoded_label'] = self.dataframe['single_label'].map(self.label_to_idx)\n",
        "\n",
        "\n",
        "        X = self.dataframe.drop('encoded_label', axis=1)\n",
        "        y = self.dataframe['encoded_label']\n",
        "\n",
        "\n",
        "        ros = RandomOverSampler()\n",
        "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "        #Combine the resampled features and labels back into a DataFrame\n",
        "        self.dataframe = X_resampled.copy()\n",
        "        self.dataframe['encoded_label'] = y_resampled\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_name = os.path.join(self.audio_dir, str(self.dataframe.iloc[idx, 0]) + '.wav')\n",
        "        waveform, sample_rate = torchaudio.load(audio_name)\n",
        "\n",
        "        #Resample if necessary\n",
        "        if sample_rate != 16000:\n",
        "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "        #Convert to Mel spectrogram\n",
        "        mel_spec = self.mel_spectrogram(waveform)\n",
        "\n",
        "        #Normalize the spectrogram to the range [0, 1]\n",
        "        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n",
        "\n",
        "        #Ensure the spectrogram has the correct shape (1 channel)\n",
        "        if mel_spec.size(0) != 1:\n",
        "            mel_spec = mel_spec.mean(dim=0, keepdim=True)  # Convert multi-channel to single-channel by averaging\n",
        "\n",
        "        #Pad or truncate the spectrogram to the max_len\n",
        "        if mel_spec.size(-1) < self.max_len:\n",
        "            padding = self.max_len - mel_spec.size(-1)\n",
        "            mel_spec = F.pad(mel_spec, (0, padding))\n",
        "        else:\n",
        "            mel_spec = mel_spec[:, :, :self.max_len]\n",
        "\n",
        "        #Retrieve the encoded label\n",
        "        label = self.dataframe.iloc[idx]['encoded_label']\n",
        "        label = torch.tensor(label).long()  #Convert label to tensor\n",
        "\n",
        "        return mel_spec, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDvY_cvBfSNg"
      },
      "outputs": [],
      "source": [
        "def pad_collate_fn(batch):\n",
        "    #Find the maximum width (time dimension) of the spectrograms in the batch\n",
        "    max_len = max([item[0].size(-1) for item in batch])  # Using size(-1) to get the last dimension\n",
        "\n",
        "    #Pad all spectrograms in the batch to have the same width\n",
        "    batch_padded = []\n",
        "    for waveform, label in batch:\n",
        "        #Padding the last dimension to the max_len\n",
        "        padded_waveform = F.pad(waveform, (0, max_len - waveform.size(-1)))\n",
        "        batch_padded.append((padded_waveform, label))\n",
        "\n",
        "    #Stack the tensors to form the batch\n",
        "    waveforms = torch.stack([item[0] for item in batch_padded])\n",
        "    labels = torch.tensor([item[1] for item in batch_padded])\n",
        "\n",
        "    return waveforms, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZvSMCIlZ_4Z"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset = FSD50KDataset(csv_file=train_csv_path, audio_dir=train_audio_dir)\n",
        "test_dataset = FSD50KDataset(csv_file=test_csv_path, audio_dir=test_audio_dir)\n",
        "\n",
        "#Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)\n",
        "'''\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    try:\n",
        "        # Attempt to transfer inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "    except RuntimeError as e:\n",
        "        # Log detailed information about the error\n",
        "        print(f\"Error with batch {i}, error: {e}\")\n",
        "\n",
        "        # Log the filenames involved in the problematic batch\n",
        "        problematic_files = train_df.iloc[i * batch_size:(i + 1) * batch_size]['fname'].values\n",
        "        print(f\"Files in this batch: {problematic_files}\")\n",
        "\n",
        "        # Skip this batch and continue with the next one\n",
        "        continue\n",
        "'''\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised\n"
      ],
      "metadata": {
        "id": "IgFGs2S_Ni0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XuL7-eBaMuO"
      },
      "outputs": [],
      "source": [
        "class AudioCNN(nn.Module):\n",
        "    def __init__(self, num_classes, max_len=1000):\n",
        "        super(AudioCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        #Update flattened size based on the output after all conv and pool layers\n",
        "        flattened_size = 256 * 2 * 31\n",
        "\n",
        "        #Define fully connected layers\n",
        "        self.fc1 = nn.Linear(flattened_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        #Flatten the tensor for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL6TRvRJXi9f"
      },
      "outputs": [],
      "source": [
        "label_distribution = train_df['single_label'].value_counts()\n",
        "print(label_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xulhmoYFXkxU"
      },
      "outputs": [],
      "source": [
        "class_weights = 1. / label_distribution\n",
        "class_weights = class_weights / class_weights.sum()  #Normalize so that weights sum to 1\n",
        "class_weights = torch.tensor(class_weights.values, dtype=torch.float32)\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJCRwKr4a8xQ"
      },
      "outputs": [],
      "source": [
        "num_classes = train_df.iloc[:, 1].nunique()\n",
        "\n",
        "num_classes = train_df.iloc[:, 1].nunique()\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "train_df = train_df[train_df['label_idx'] < num_classes]\n",
        "\n",
        "max_label_in_test = test_df['label_idx'].max()\n",
        "min_label_in_test = test_df['label_idx'].min()\n",
        "print(f\"Max label in test set: {max_label_in_test}, Min label: {min_label_in_test}\")\n",
        "assert max_label_in_test < num_classes, \"Test set has out-of-bounds labels.\"\n",
        "\n",
        "test_df = test_df[test_df['label_idx'] < num_classes]\n",
        "\n",
        "#Re-create the DataLoader with the filtered dataset\n",
        "train_dataset = FSD50KDataset(csv_file=train_csv_sampled_path, audio_dir=train_audio_dir)\n",
        "test_dataset = FSD50KDataset(csv_file=test_csv_sampled_path, audio_dir=test_audio_dir)\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Max label in train set: {train_df['label_idx'].max()}, Min label: {train_df['label_idx'].min()}\")\n",
        "print(f\"Max label in test set: {max_label_in_test}, Min label: {min_label_in_test}\")\n",
        "\n",
        "#Check if there's any label in the training or test set that is out of bounds\n",
        "out_of_bounds_train = train_df[train_df['label_idx'] >= num_classes]\n",
        "out_of_bounds_test = test_df[test_df['label_idx'] >= num_classes]\n",
        "\n",
        "print(f\"Number of out-of-bounds labels in train set: {len(out_of_bounds_train)}\")\n",
        "print(f\"Number of out-of-bounds labels in test set: {len(out_of_bounds_test)}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTj65s20Ieay"
      },
      "outputs": [],
      "source": [
        "#Initialize model, loss function, and optimizer\n",
        "model = AudioCNN(num_classes)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40_k1uKlYFIq"
      },
      "outputs": [],
      "source": [
        "num_batches = len(train_loader)\n",
        "print(f'Total number of batches: {num_batches}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXTgFQmkbBzR"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "print(f\"Before batching, unique label indices:\", train_df['label_idx'].unique())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        #Debugging print for labels in each batch\n",
        "        print(f\"Batch {i} - Labels: {labels.tolist()}\")\n",
        "\n",
        "        #Ensure labels are within the correct range\n",
        "        if any(lbl >= num_classes for lbl in labels):\n",
        "            print(f\"Error: Found label {lbl.item()} which is out of bounds!\")\n",
        "            print(f\"Batch {i} - Inputs shape: {inputs.shape}\")\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        #Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    #Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "print(f\"Final learning rate: {scheduler.get_last_lr()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIklDXfKoX_q"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/MyDrive/Thesis/FSD50K/trained_audio_cnn_model5.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEYfTGVspt7w"
      },
      "outputs": [],
      "source": [
        "print(f\"Max label in test set: {test_df['label_idx'].max()}\")\n",
        "print(f\"Number of classes the model was trained on: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l65ylJZuqNJC"
      },
      "outputs": [],
      "source": [
        "print(f\"Unique labels in test set: {sorted(test_df['label_idx'].unique())}\")\n",
        "print(f\"Unique labels in training set: {sorted(train_df['label_idx'].unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4siRxm9eCZJ"
      },
      "outputs": [],
      "source": [
        "model = AudioCNN(num_classes)  #Adjust the number of classes if necessary\n",
        "model.load_state_dict(torch.load(model_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qf2serOc3vm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "total_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probs = []  #To store the predicted probabilities for mAP calculation\n",
        "\n",
        "with torch.no_grad():  #Disable gradient computation for evaluation\n",
        "    for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
        "\n",
        "        valid_indices = labels < num_classes\n",
        "        inputs = inputs[valid_indices]\n",
        "        labels = labels[valid_indices]\n",
        "\n",
        "        if len(labels) == 0:\n",
        "            continue\n",
        "\n",
        "        #Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        #Store all labels and predictions for metric calculation\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "#Calculate average loss and accuracy\n",
        "average_loss = total_loss / total_samples\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "#Calculate precision, recall, and f1 score\n",
        "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "#Calculate mAP (One-vs-Rest)\n",
        "all_labels_one_hot = np.eye(num_classes)[all_labels]\n",
        "mAP = average_precision_score(all_labels_one_hot, all_probs, average='macro')\n",
        "\n",
        "#Print metrics\n",
        "print(f'Average loss: {average_loss:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'mAP (One-vs-Rest): {mAP:.4f}')\n",
        "\n",
        "#Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOZyq-TND9wc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_predictions, pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeNWS2p9Eakk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(all_labels, all_predictions)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raWqoepfzv0k"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_distribution = Counter(all_labels)\n",
        "prediction_distribution = Counter(all_predictions)\n",
        "\n",
        "print(\"Label Distribution:\", label_distribution)\n",
        "print(\"Prediction Distribution:\", prediction_distribution)\n",
        "\n",
        "#Plot the class distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(label_distribution.keys(), label_distribution.values(), color='blue')\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(prediction_distribution.keys(), prediction_distribution.values(), color='red')\n",
        "plt.title(\"Prediction Distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siNpiGDxkSzZ"
      },
      "source": [
        "# VGGish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vy7o1Maka5B"
      },
      "outputs": [],
      "source": [
        "!pip install torchvggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_2lOWYEkUly"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import Resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5aveZ40k2w9"
      },
      "outputs": [],
      "source": [
        "class FSD50KDataset(Dataset):\n",
        "    def __init__(self, csv_file, audio_dir, transform=None, max_len=1000):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "        self.max_len = max_len\n",
        "\n",
        "        #Encode string labels to integers\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(self.dataframe['single_label'].unique())}\n",
        "        self.dataframe['encoded_label'] = self.dataframe['single_label'].map(label_to_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_name = os.path.join(self.audio_dir, str(self.dataframe.iloc[idx, 0]) + '.wav')\n",
        "        waveform, sample_rate = torchaudio.load(audio_name)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate, n_mels=64, n_fft=1024, hop_length=512\n",
        "        )(waveform)\n",
        "\n",
        "        #Ensure the spectrogram has the correct shape (1 channel)\n",
        "        mel_spec = mel_spec.mean(dim=0, keepdim=True)\n",
        "\n",
        "        #Normalize the spectrogram\n",
        "        mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()\n",
        "\n",
        "        #Pad or truncate the spectrogram to the max_len\n",
        "        if mel_spec.size(-1) < self.max_len:\n",
        "            padding = self.max_len - mel_spec.size(-1)\n",
        "            mel_spec = F.pad(mel_spec, (0, padding))\n",
        "        else:\n",
        "            mel_spec = mel_spec[:, :, :self.max_len]\n",
        "\n",
        "        mel_spec = mel_spec.repeat(3, 1, 1)\n",
        "\n",
        "        label = self.dataframe.iloc[idx]['encoded_label']\n",
        "        label = torch.tensor(label).long()  # Convert label to tensor\n",
        "\n",
        "        return mel_spec, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkhpimmTk-hC"
      },
      "outputs": [],
      "source": [
        "class VGGishClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGishClassifier, self).__init__()\n",
        "        self.vggish = models.vgg16(pretrained=True)\n",
        "\n",
        "        #Freeze the VGGish layers\n",
        "        for param in self.vggish.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        #Replace the classifier to fit your task\n",
        "        self.vggish.classifier[-1] = nn.Linear(self.vggish.classifier[-1].in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vggish(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzO3l5jYA6xh"
      },
      "outputs": [],
      "source": [
        "train_dataset = FSD50KDataset(csv_file=train_csv_path, audio_dir=train_audio_dir)\n",
        "test_dataset = FSD50KDataset(csv_file=test_csv_path, audio_dir=test_audio_dir)\n",
        "\n",
        "#Identifying and keeping top n classes\n",
        "class_distribution = train_dataset.dataframe['encoded_label'].value_counts()\n",
        "top_n_classes = class_distribution.head(50).index\n",
        "filtered_df = train_dataset.dataframe[train_dataset.dataframe['encoded_label'].isin(top_n_classes)]\n",
        "train_dataset.dataframe = filtered_df.reset_index(drop=True)\n",
        "\n",
        "#Recalculate number of classes and remap labels\n",
        "num_classes = len(filtered_df['encoded_label'].unique())\n",
        "label_to_idx = {label: idx for idx, label in enumerate(filtered_df['encoded_label'].unique())}\n",
        "train_dataset.dataframe['encoded_label'] = train_dataset.dataframe['encoded_label'].map(label_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWQBCGErlCZe"
      },
      "outputs": [],
      "source": [
        "\n",
        "#DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3AKl1WqqJPC"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.dataframe.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5BZrC9SlLso"
      },
      "outputs": [],
      "source": [
        "num_classes = len(train_dataset.dataframe['encoded_label'].unique())\n",
        "print(num_classes)\n",
        "\n",
        "model = VGGishClassifier(num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyMVhbpL6sCE"
      },
      "outputs": [],
      "source": [
        "print(f\"Unique labels in training data: {train_dataset.dataframe['encoded_label'].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IXnf27Uo5F0"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum label value:\", train_dataset.dataframe['encoded_label'].min())\n",
        "print(\"Maximum label value:\", train_dataset.dataframe['encoded_label'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpRfDAHyo6pY"
      },
      "outputs": [],
      "source": [
        "unique_labels = train_dataset.dataframe['encoded_label'].unique()\n",
        "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
        "print(f\"Number of unique labels: {len(unique_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fbRchFJFpLqA"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in train_loader:\n",
        "    print(f\"Sample inputs: {inputs}\")\n",
        "    print(f\"Sample labels: {labels}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTwtoZsGlPLI"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "#Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        #Clip gradients to prevent exploding gradients\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfqC-CBixOX2"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install torchvision\n",
        "!pip install torch-audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUldq2Qr45jI"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZwkNa5y9CRT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV39SJgv88pc"
      },
      "outputs": [],
      "source": [
        "class FSD50KDataset(Dataset):\n",
        "    def __init__(self, csv_file, audio_dir, transform=None, max_len=1000):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "        self.max_len = max_len\n",
        "\n",
        "        #Encode string labels to integers\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(self.dataframe['single_label'].unique())}\n",
        "        self.dataframe['encoded_label'] = self.dataframe['single_label'].map(label_to_idx)\n",
        "\n",
        "        #Filter to keep only the top 50 classes\n",
        "        top_n_classes = self.dataframe['encoded_label'].value_counts().head(50).index\n",
        "        self.dataframe = self.dataframe[self.dataframe['encoded_label'].isin(top_n_classes)]\n",
        "        self.dataframe.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        #Re-encode labels after filtering\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(self.dataframe['encoded_label'].unique())}\n",
        "        self.dataframe['encoded_label'] = self.dataframe['encoded_label'].map(label_to_idx)\n",
        "\n",
        "        #Separate features and labels for oversampling\n",
        "        X = self.dataframe.drop('encoded_label', axis=1)\n",
        "        y = self.dataframe['encoded_label']\n",
        "\n",
        "        #Perform oversampling\n",
        "        ros = RandomOverSampler()\n",
        "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "        #Combine the resampled features and labels back into a DataFrame\n",
        "        self.dataframe = X_resampled.copy()\n",
        "        self.dataframe['encoded_label'] = y_resampled\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_name = os.path.join(self.audio_dir, str(self.dataframe.iloc[idx, 0]) + '.wav')\n",
        "        waveform, sample_rate = torchaudio.load(audio_name)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate, n_mels=64, n_fft=1024, hop_length=512)(waveform)\n",
        "\n",
        "        mel_spec = mel_spec.mean(dim=0, keepdim=True)\n",
        "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6)\n",
        "\n",
        "        if mel_spec.size(-1) < self.max_len:\n",
        "            padding = self.max_len - mel_spec.size(-1)\n",
        "            mel_spec = nn.functional.pad(mel_spec, (0, padding))\n",
        "        else:\n",
        "            mel_spec = mel_spec[:, :, :self.max_len]\n",
        "\n",
        "        mel_spec = nn.functional.interpolate(mel_spec.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        mel_spec = mel_spec.repeat(3, 1, 1)\n",
        "\n",
        "        label = torch.tensor(self.dataframe.iloc[idx]['encoded_label']).long()\n",
        "        return mel_spec, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpFpIrc9K_G"
      },
      "outputs": [],
      "source": [
        "class VGGishClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGishClassifier, self).__init__()\n",
        "        self.vggish = models.vgg16(pretrained=True)\n",
        "\n",
        "        for param in self.vggish.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.vggish.classifier[-1] = nn.Linear(self.vggish.classifier[-1].in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vggish(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU48zZXFPYTt"
      },
      "outputs": [],
      "source": [
        "def set_bn_eval(m):\n",
        "    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        m.eval()\n",
        "        m.weight.requires_grad = False\n",
        "        m.bias.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-yEiLBM9MOr"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "model = VGGishClassifier(num_classes=50)\n",
        "\n",
        "model.apply(set_bn_eval)\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-4\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = FSD50KDataset(csv_file=train_csv_sampled_path, audio_dir=train_audio_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTAgk9XeX6ME"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "labels = train_dataset.dataframe['encoded_label']\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyLjbGuKQN8r"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TENQA6M-HJJ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lhI6luvyTSw"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/MyDrive/Thesis/FSD50K/vggish_trained_audio_model.pth' #\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9NxhTo0qOgU"
      },
      "outputs": [],
      "source": [
        "test_dataset = FSD50KDataset(csv_file=test_csv_path, audio_dir=test_audio_dir) #\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAR6MrsMrXBt"
      },
      "outputs": [],
      "source": [
        "unique_labels = test_dataset.dataframe['encoded_label'].unique()\n",
        "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
        "print(f\"Number of unique labels: {len(unique_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUsBKel-0BkN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i2NnL31-44LV"
      },
      "outputs": [],
      "source": [
        "class_names = test_dataset.dataframe['single_label'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWOXq56opvel"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average=None)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"Class: {class_name} - Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}, F1-Score: {f1_score[i]:.2f}\")\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro')\n",
        "print(f\"Overall (Macro) - Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1_score:.2f}\")\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
        "print(f\"Overall (Weighted) - Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1_score:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzGorCbECLT-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_predictions, pos_label=1)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-TmPmj_CY35"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF8u2uHyCgzl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(all_labels, all_predictions)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sly8Ge9dMyL"
      },
      "source": [
        "# YAMNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sf6YMPIsdPnL"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c7oD3nBdShW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRc25k3qdX-n"
      },
      "outputs": [],
      "source": [
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faGvSebrdcGt"
      },
      "outputs": [],
      "source": [
        "#CSV data\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df = train_df.sample(n=20483, random_state=42)\n",
        "train_filenames = train_df['fname'].astype(str).values\n",
        "\n",
        "import librosa\n",
        "\n",
        "def load_and_preprocess_audio(filename, label):\n",
        "    if isinstance(filename, tf.Tensor):\n",
        "        filename = tf.compat.as_str_any(filename.numpy())\n",
        "\n",
        "    audio_path = os.path.join(train_audio_dir, filename + '.wav')\n",
        "\n",
        "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    waveform = (waveform - waveform.mean()) / waveform.max()\n",
        "\n",
        "    if len(waveform) < 16000:\n",
        "        waveform = np.pad(waveform, (0, 16000 - len(waveform)), mode='constant')\n",
        "    elif len(waveform) > 16000:\n",
        "        waveform = waveform[:16000]\n",
        "\n",
        "    return tf.convert_to_tensor(waveform, dtype=tf.float32), label\n",
        "\n",
        "def load_and_preprocess_audio_wrapper(filename, label):\n",
        "    waveform, label = tf.py_function(\n",
        "        load_and_preprocess_audio,\n",
        "        [filename, label],\n",
        "        [tf.float32, tf.int32]\n",
        "    )\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    return waveform, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB8jZbgir1y5"
      },
      "outputs": [],
      "source": [
        "print(train_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0fATT3br5Gc"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "train_df['encoded_label'] = le.fit_transform(train_df['single_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrlFpoE3iTTs"
      },
      "outputs": [],
      "source": [
        "train_filenames = train_df['fname'].values\n",
        "train_labels = train_df['encoded_label'].values\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels.astype(np.int32)))\n",
        "train_dataset = train_dataset.map(load_and_preprocess_audio_wrapper)\n",
        "train_loader = train_dataset.shuffle(buffer_size=1024).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQy3U9aMeFdb"
      },
      "outputs": [],
      "source": [
        "class YAMNetClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(YAMNetClassifier, self).__init__()\n",
        "        self.yamnet_model = hub.KerasLayer(\"https://tfhub.dev/google/yamnet/1\", trainable=False)\n",
        "        self.global_avg_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.fc = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        embeddings, _, _ = self.yamnet_model(inputs)  #Get embeddings from YAMNet\n",
        "        pooled_embeddings = self.global_avg_pool(embeddings)  #Pool the embeddings\n",
        "        return self.fc(pooled_embeddings)  #Pass through final dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS-sbnGs5eeC"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        print(\"Input shape:\", inputs.shape)\n",
        "        return self.fc(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kURbUBBfXbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "train_df['encoded_label'] = le.fit_transform(train_df['single_label'])\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5FRS1MG8eJT"
      },
      "outputs": [],
      "source": [
        "print(train_df['encoded_label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rxBuSqD9BvI"
      },
      "outputs": [],
      "source": [
        "model = YAMNetClassifier(num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30llkSX8fhxH"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZmtNiSkzd8d"
      },
      "outputs": [],
      "source": [
        "for data, label in train_loader.take(1):\n",
        "    print(data.shape, label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut1-kyWE6DDY"
      },
      "outputs": [],
      "source": [
        "sample_data, sample_label = next(iter(train_loader))\n",
        "output = model(sample_data)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgWL_rQg7IBv"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow Hub version:\", hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwiPpQnG_4xG"
      },
      "outputs": [],
      "source": [
        "single_sample_data = tf.reshape(single_sample_data, [1, 16000])\n",
        "\n",
        "#Ensure data type is float32\n",
        "single_sample_data = tf.cast(single_sample_data, dtype=tf.float32)\n",
        "\n",
        "#Pass through YAMNet\n",
        "output = model(single_sample_data)\n",
        "\n",
        "#Print output shape\n",
        "print(f\"Single sample model output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RjRqBSH-jB8"
      },
      "outputs": [],
      "source": [
        "sample_data, _ = next(iter(train_loader))\n",
        "print(f\"Sample data shape before model: {sample_data.shape}\")\n",
        "\n",
        "#Ensure correct shape and dtype\n",
        "sample_data = tf.ensure_shape(sample_data, (None, 16000))\n",
        "sample_data = tf.cast(sample_data, tf.float32)\n",
        "\n",
        "#Pass through YAMNet model\n",
        "output = model(sample_data)\n",
        "print(f\"Model output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjG1jfttynmc"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for step, (data, labels) in enumerate(train_loader):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(data)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {tf.reduce_mean(loss).numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Yc-82b88QR"
      },
      "source": [
        "## Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppuJCJPKUqT0"
      },
      "outputs": [],
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        #Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        #Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr_hr8YNUsHK"
      },
      "outputs": [],
      "source": [
        "class FSD50KDatasetUSP(Dataset):\n",
        "    def __init__(self, csv_file, audio_dir, transform=None):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_name = os.path.join(self.audio_dir, str(self.dataframe.iloc[idx, 0]) + '.wav')\n",
        "        waveform, sample_rate = torchaudio.load(audio_name)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "            #Normalize the waveform to the range [0, 1]\n",
        "            waveform = (waveform - waveform.min()) / (waveform.max() - waveform.min())\n",
        "\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rICT-yXcLdP"
      },
      "outputs": [],
      "source": [
        "def pad_collate_fn(batch):\n",
        "    #Find the longest sequence in the batch\n",
        "    max_len = max(item[0].size(-1) for item in batch)\n",
        "\n",
        "    #Pad the sequences to the same length\n",
        "    padded_batch = []\n",
        "    for data in batch:\n",
        "        waveform, = data\n",
        "        padding = max_len - waveform.size(-1)\n",
        "        if padding > 0:\n",
        "            waveform = F.pad(waveform, (0, padding), \"constant\", 0)\n",
        "        padded_batch.append((waveform,))\n",
        "\n",
        "    #Stack the padded sequences\n",
        "    batch = torch.stack([item[0] for item in padded_batch])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eHC6-W8pTVG"
      },
      "outputs": [],
      "source": [
        "label_distribution = train_df['single_label'].value_counts()\n",
        "print(label_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJCJq9cHp17N"
      },
      "outputs": [],
      "source": [
        "class_weights = 1. / label_distribution\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "class_weights = torch.tensor(class_weights.values, dtype=torch.float32)\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsO5L20hU4OS"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "transform = transforms.MelSpectrogram(sample_rate=16000, n_fft=1024, hop_length=512)\n",
        "train_dataset = FSD50KDatasetUSP(csv_file=train_csv_sampled_path, audio_dir=train_audio_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXNWGgSZVB5u"
      },
      "outputs": [],
      "source": [
        "model = ConvAutoencoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn7yJrVzXEGX"
      },
      "outputs": [],
      "source": [
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data[0]\n",
        "\n",
        "        #Reshape to the correct shape: (batch_size, 1, height, width)\n",
        "        data = data.view(data.size(0), 1, data.size(-2), data.size(-1))  # Reshape appropriately\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "\n",
        "\n",
        "        #Pad the input to match the size of the output\n",
        "        if outputs.size(-1) > data.size(-1):\n",
        "            padding = outputs.size(-1) - data.size(-1)\n",
        "            data = F.pad(data, (0, padding))\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        print(f\"Batch {batch_idx} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Learning Rate: {scheduler.get_last_lr()[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1o1GeQ79fB2"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/MyDrive/Thesis/FSD50K/autoencoder_model5.pth'\n",
        "\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWg3AFlDTn9G"
      },
      "outputs": [],
      "source": [
        "test_dataset = FSD50KDatasetUSP(csv_file=test_csv_sampled_path, audio_dir=test_audio_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbsBoTl-JPcJ"
      },
      "outputs": [],
      "source": [
        "autoencoder = ConvAutoencoder()\n",
        "autoencoder.load_state_dict(torch.load('/content/drive/MyDrive/Thesis/FSD50K/autoencoder_model5.pth'))\n",
        "autoencoder.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-0b2s5CO5MD"
      },
      "outputs": [],
      "source": [
        "label_distribution = test_df['single_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbKLgW4bJVpx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_features_from_autoencoder(autoencoder, dataloader):\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs = data[0] if isinstance(data, tuple) else data\n",
        "\n",
        "            if len(inputs.shape) == 3:\n",
        "                inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            print(f\"Input type: {type(inputs)}, Input shape: {inputs.shape}\")\n",
        "\n",
        "            encoded_features = autoencoder.encoder(inputs)\n",
        "\n",
        "            flat_features = encoded_features.view(encoded_features.size(0), -1)\n",
        "            print(f\"Flat features shape: {flat_features.shape}\")\n",
        "\n",
        "            features.append(flat_features.numpy())\n",
        "\n",
        "    #Ensure all features have the same shape\n",
        "    min_shape = min(f.shape[1] for f in features)\n",
        "    features = [f[:, :min_shape] for f in features]\n",
        "\n",
        "    return np.concatenate(features, axis=0)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "true_labels = test_df['single_label'].values\n",
        "\n",
        "#Encode the string labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "true_labels_encoded = label_encoder.fit_transform(true_labels)\n",
        "\n",
        "#Extract features from the test set\n",
        "test_features = extract_features_from_autoencoder(autoencoder, test_loader)\n",
        "\n",
        "#Apply K-Means clustering\n",
        "n_clusters = 332\n",
        "print(\"Starting K-Means clustering...\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(test_features)\n",
        "\n",
        "#Get the cluster labels\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "print(f\"Cluster labels: {cluster_labels}\")\n",
        "\n",
        "ari = adjusted_rand_score(true_labels_encoded, cluster_labels)\n",
        "nmi = normalized_mutual_info_score(true_labels_encoded, cluster_labels)\n",
        "print(f\"Adjusted Rand Index: {ari}\")\n",
        "print(f\"Normalized Mutual Information: {nmi}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb_jgYXe6lar"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "silhouette_avg = silhouette_score(test_features, cluster_labels)\n",
        "sample_silhouette_values = silhouette_samples(test_features, cluster_labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(sample_silhouette_values, bins=50)\n",
        "plt.title('Silhouette Scores')\n",
        "plt.xlabel('Silhouette Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2HmszCqEuox"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "test_features_2d = tsne.fit_transform(test_features)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(test_features_2d[:, 0], test_features_2d[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.5)\n",
        "plt.colorbar()\n",
        "plt.title('t-SNE plot of Clusters')\n",
        "plt.xlabel('t-SNE component 1')\n",
        "plt.ylabel('t-SNE component 2')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}